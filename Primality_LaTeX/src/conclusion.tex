\section{Conclusions}
%We didn't conclude anything just yet. %TODO

%For small numbers, simple techniques like trial division seem to work well.
%For big numbers, probablistic method seems to perform better.

We have looked at the behaviour of different common primality tests as a function of the input size.
Simple deterministic algorithms (e.g., trial division) work very well for small numbers but become slow when applied to large numbers.

Randomized algorithms scale well with their inputs and have small error percentages.
While they are slower than the deterministic algorithms for small numbers,
for numbers greater than $\approx 2^{10}$ they outperform the tested simple deterministic algorithms.
They also have the nice property that they can be repeated multiple times when even smaller errors are required: pay time to gain a lower error.

AKS might be nice for theoretical reasons (i.e., it proves that primality testing is in $P$), but in practice the algorithm is rather slow.
Maybe the algorithm may be optimized, but in our current implementation, it is more than a factor $10^9$ slower than it can be,
because it performs very poorly when it is testing a prime number.
With our current implementation, it doesn't seem feasible for practical purposes.
